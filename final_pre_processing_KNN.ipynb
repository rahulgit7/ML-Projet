{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation, NMF, FactorAnalysis\n",
    "from sklearn.cluster import KMeans, SpectralClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from sklearn.metrics import mean_squared_error, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_hamming(data):\n",
    "    categories_dist = []\n",
    "\n",
    "    for category in data:\n",
    "        print(f'categories: {category}')\n",
    "        X = pd.get_dummies(data[category])  #applying one-hot encoding - categorical value to binary vectors\n",
    "        X_mean = X * X.mean() #weighted binary values\n",
    "        X_dot = X_mean.dot(X.transpose()) #calculating dot product - similarity btw different rows\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)  #store all similarity matrices for each categorical variable\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)  #computing harmonic mean - single weighted hamming distance matrix (sensitive to dissimilarities)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Matrix Function\n",
    "def distance_matrix(data, numeric_distance=\"euclidean\", categorical_distance=\"jaccard\"):\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    if is_mixed_type:\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0])\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0 * (result_numeric[i, j] * len(data_numeric.columns) + result_categorical[i, j] *\n",
    "                                len(data_categorical.columns)) / len(data.columns) for j in range(len(data))] for i in range(len(data))])\n",
    "\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Imputation with Distance Matrix\n",
    "def knn_impute(data, numeric_distance=\"euclidean\", categorical_distance=\"jaccard\", k_neighbors=1):\n",
    "    distances = distance_matrix(data, numeric_distance, categorical_distance)\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().any():\n",
    "            for i in data[column][data[column].isnull()].index:\n",
    "                nearest_neighbors = distances.loc[i].nsmallest(k_neighbors + 1).iloc[1:].index\n",
    "                if pd.api.types.is_numeric_dtype(data[column]):\n",
    "                    data.at[i, column] = data[column].loc[nearest_neighbors].mean()\n",
    "                else:\n",
    "                    data.at[i, column] = data[column].loc[nearest_neighbors].mode()[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline\n",
    "with open('/Users/saikrishnakathika/Documents/DATA_245_Machine_Learning/project/code/data.pickle', 'rb') as handle:\n",
    "    subs_train, subs_test, subs_val, hlth_train, hlth_test, hlth_val, demo_train, demo_test, demo_val = pickle.load(handle)\n",
    "\n",
    "# Combine data subsets for imputation\n",
    "all_train = pd.concat([subs_train, hlth_train, demo_train], axis=1)\n",
    "all_test = pd.concat([subs_test, hlth_test, demo_test], axis=1)\n",
    "all_val = pd.concat([subs_val, hlth_val, demo_val], axis=1)\n",
    "\n",
    "# Combine all_test with all_val\n",
    "all_test = pd.concat([all_test, all_val], axis=0)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "all_train = all_train.drop(['QUESTID2', 'FILEDATE'], axis=1, errors='ignore')\n",
    "all_test = all_test.drop(['QUESTID2', 'FILEDATE'], axis=1, errors='ignore')\n",
    "\n",
    "subs_train = subs_train.drop(['QUESTID2', 'FILEDATE'], axis=1, errors='ignore')\n",
    "subs_test = subs_test.drop(['QUESTID2', 'FILEDATE'], axis=1, errors='ignore')\n",
    "subs_val = subs_val.drop(['QUESTID2', 'FILEDATE'], axis=1, errors='ignore')\n",
    "\n",
    "# Drop columns with specific patterns (e.g., regex 'II')\n",
    "all_train = all_train.drop(all_train.filter(regex='II').columns, axis=1)\n",
    "all_test = all_test.drop(all_test.filter(regex='II').columns, axis=1)\n",
    "\n",
    "subs_train = subs_train.drop(subs_train.filter(regex='II').columns, axis=1)\n",
    "hlth_train = hlth_train.drop(hlth_train.filter(regex='II').columns, axis=1)\n",
    "demo_train = demo_train.drop(demo_train.filter(regex='II').columns, axis=1)\n",
    "\n",
    "subs_test = subs_test.drop(subs_test.filter(regex='II').columns, axis=1)\n",
    "hlth_test = hlth_test.drop(hlth_test.filter(regex='II').columns, axis=1)\n",
    "demo_test = demo_test.drop(demo_test.filter(regex='II').columns, axis=1)\n",
    "\n",
    "subs_val = subs_val.drop(subs_val.filter(regex='II').columns, axis=1)\n",
    "hlth_val = hlth_val.drop(hlth_val.filter(regex='II').columns, axis=1)\n",
    "demo_val = demo_val.drop(demo_val.filter(regex='II').columns, axis=1)\n",
    "\n",
    "# Replace invalid values (>=80) with NaN\n",
    "all_train[all_train >= 80] = np.nan\n",
    "all_test[all_test >= 80] = np.nan\n",
    "\n",
    "subs_train[subs_train >= 80] = np.nan\n",
    "hlth_train[hlth_train >= 80] = np.nan\n",
    "demo_train[demo_train >= 80] = np.nan\n",
    "\n",
    "subs_test[subs_test >= 80] = np.nan\n",
    "hlth_test[hlth_test >= 80] = np.nan\n",
    "demo_test[demo_test >= 80] = np.nan\n",
    "\n",
    "subs_val[subs_val >= 80] = np.nan\n",
    "hlth_val[hlth_val >= 80] = np.nan\n",
    "demo_val[demo_val >= 80] = np.nan\n",
    "\n",
    "# Drop columns with >80 missing values\n",
    "all_train = all_train.dropna(axis=1, thresh=int(all_train.shape[0] / 5))\n",
    "cols1 = [c for c in all_train.columns]\n",
    "all_test = all_test[cols1]\n",
    "\n",
    "subs_train = subs_train.dropna(axis=1, thresh=int(subs_train.shape[0] / 5))\n",
    "cols2 = [c for c in subs_train.columns]\n",
    "subs_test = subs_test[cols2]\n",
    "subs_val = subs_val[cols2]\n",
    "\n",
    "hlth_train = hlth_train.dropna(axis=1, thresh=int(hlth_train.shape[0] / 5))\n",
    "cols3 = [c for c in hlth_train.columns]\n",
    "hlth_test = hlth_test[cols3]\n",
    "hlth_val = hlth_val[cols3]\n",
    "\n",
    "demo_train = demo_train.dropna(axis=1, thresh=int(demo_train.shape[0] / 5))\n",
    "cols4 = [c for c in demo_train.columns]\n",
    "demo_test = demo_test[cols4]\n",
    "demo_val = demo_val[cols4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KNN Imputation on all subsets\n",
    "all_train = knn_impute(all_train, k_neighbors=1)\n",
    "all_test = knn_impute(all_test, k_neighbors=1)\n",
    "\n",
    "subs_train = knn_impute(subs_train, k_neighbors=1)\n",
    "subs_test = knn_impute(subs_test, k_neighbors=1)\n",
    "subs_val = knn_impute(subs_val, k_neighbors=1)\n",
    "\n",
    "hlth_train = knn_impute(hlth_train, k_neighbors=1)\n",
    "hlth_test = knn_impute(hlth_test, k_neighbors=1)\n",
    "hlth_val = knn_impute(hlth_val, k_neighbors=1)\n",
    "\n",
    "demo_train = knn_impute(demo_train, k_neighbors=1)\n",
    "demo_test = knn_impute(demo_test, k_neighbors=1)\n",
    "demo_val = knn_impute(demo_val, k_neighbors=1)\n",
    "\n",
    "# Save all datasets to a pickle file\n",
    "with open('data_removed_updated.pickle', 'wb') as handle:\n",
    "    pickle.dump([all_train, all_test, subs_train, subs_test, subs_val, hlth_train, hlth_test, hlth_val, demo_train, demo_test, demo_val], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45020 entries, 44412 to 30140\n",
      "Data columns (total 35 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   CIGEVER    45020 non-null  int64  \n",
      " 1   CIGOFRSM   45020 non-null  float64\n",
      " 2   CIGWILYR   45020 non-null  float64\n",
      " 3   CIGTRY     45020 non-null  float64\n",
      " 4   CIGREC     45020 non-null  float64\n",
      " 5   CIGDLYMO   45020 non-null  float64\n",
      " 6   CIGAGE     45020 non-null  float64\n",
      " 7   CIG100LF   45020 non-null  float64\n",
      " 8   SMKLSSEVR  45020 non-null  float64\n",
      " 9   CIGAREVR   45020 non-null  float64\n",
      " 10  CIGARTRY   45020 non-null  float64\n",
      " 11  CIGARREC   45020 non-null  float64\n",
      " 12  PIPEVER    45020 non-null  float64\n",
      " 13  ALCEVER    45020 non-null  float64\n",
      " 14  ALCTRY     45020 non-null  float64\n",
      " 15  ALCREC     45020 non-null  float64\n",
      " 16  ALCYRTOT   45020 non-null  float64\n",
      " 17  ALBSTWAY   45020 non-null  float64\n",
      " 18  ALDAYPMO   45020 non-null  float64\n",
      " 19  ALDAYPWK   45020 non-null  float64\n",
      " 20  ALCDAYS    45020 non-null  float64\n",
      " 21  ALCUS30D   45020 non-null  float64\n",
      " 22  ALCBNG30D  45020 non-null  float64\n",
      " 23  MJEVER     45020 non-null  float64\n",
      " 24  MJAGE      45020 non-null  float64\n",
      " 25  MJREC      45020 non-null  float64\n",
      " 26  COCEVER    45020 non-null  float64\n",
      " 27  HEREVER    45020 non-null  float64\n",
      " 28  METHAMEVR  45020 non-null  float64\n",
      " 29  PNRANYLIF  45020 non-null  float64\n",
      " 30  PNRANYREC  45020 non-null  float64\n",
      " 31  TRQANYLIF  45020 non-null  float64\n",
      " 32  STMANYLIF  45020 non-null  float64\n",
      " 33  SEDANYLIF  45020 non-null  float64\n",
      " 34  PNRNMLIF   45020 non-null  float64\n",
      "dtypes: float64(34), int64(1)\n",
      "memory usage: 12.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIGEVER</th>\n",
       "      <th>CIGOFRSM</th>\n",
       "      <th>CIGWILYR</th>\n",
       "      <th>CIGTRY</th>\n",
       "      <th>CIGREC</th>\n",
       "      <th>CIGDLYMO</th>\n",
       "      <th>CIGAGE</th>\n",
       "      <th>CIG100LF</th>\n",
       "      <th>SMKLSSEVR</th>\n",
       "      <th>CIGAREVR</th>\n",
       "      <th>...</th>\n",
       "      <th>MJREC</th>\n",
       "      <th>COCEVER</th>\n",
       "      <th>HEREVER</th>\n",
       "      <th>METHAMEVR</th>\n",
       "      <th>PNRANYLIF</th>\n",
       "      <th>PNRANYREC</th>\n",
       "      <th>TRQANYLIF</th>\n",
       "      <th>STMANYLIF</th>\n",
       "      <th>SEDANYLIF</th>\n",
       "      <th>PNRNMLIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44412</th>\n",
       "      <td>1</td>\n",
       "      <td>3.83617</td>\n",
       "      <td>3.8709</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.956857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.377809</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31424</th>\n",
       "      <td>2</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>15.876284</td>\n",
       "      <td>2.745063</td>\n",
       "      <td>2.288681</td>\n",
       "      <td>17.956857</td>\n",
       "      <td>2.120717</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.377809</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.479697</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.164689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27959</th>\n",
       "      <td>1</td>\n",
       "      <td>3.83617</td>\n",
       "      <td>3.8709</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.479697</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.164689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>1</td>\n",
       "      <td>3.83617</td>\n",
       "      <td>3.8709</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.956857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.377809</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.479697</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.164689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41259</th>\n",
       "      <td>1</td>\n",
       "      <td>3.83617</td>\n",
       "      <td>3.8709</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.479697</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.164689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CIGEVER  CIGOFRSM  CIGWILYR     CIGTRY    CIGREC  CIGDLYMO     CIGAGE  \\\n",
       "44412        1   3.83617    3.8709  16.000000  4.000000  2.000000  17.956857   \n",
       "31424        2   4.00000    4.0000  15.876284  2.745063  2.288681  17.956857   \n",
       "27959        1   3.83617    3.8709  13.000000  3.000000  1.000000  14.000000   \n",
       "4062         1   3.83617    3.8709  21.000000  4.000000  2.000000  17.956857   \n",
       "41259        1   3.83617    3.8709  14.000000  1.000000  5.000000  14.000000   \n",
       "\n",
       "       CIG100LF  SMKLSSEVR  CIGAREVR  ...     MJREC  COCEVER  HEREVER  \\\n",
       "44412  2.000000        2.0       2.0  ...  2.377809      2.0      2.0   \n",
       "31424  2.120717        2.0       2.0  ...  2.377809      2.0      2.0   \n",
       "27959  1.000000        1.0       1.0  ...  3.000000      1.0      2.0   \n",
       "4062   2.000000        2.0       2.0  ...  2.377809      2.0      2.0   \n",
       "41259  5.000000        2.0       2.0  ...  1.000000      2.0      2.0   \n",
       "\n",
       "       METHAMEVR  PNRANYLIF  PNRANYREC  TRQANYLIF  STMANYLIF  SEDANYLIF  \\\n",
       "44412        2.0        1.0   2.000000        2.0        2.0        2.0   \n",
       "31424        2.0        2.0   1.479697        2.0        2.0        2.0   \n",
       "27959        2.0        2.0   1.479697        2.0        5.0        2.0   \n",
       "4062         2.0        2.0   1.479697        2.0        2.0        2.0   \n",
       "41259        2.0        2.0   1.479697        2.0        2.0        2.0   \n",
       "\n",
       "       PNRNMLIF  \n",
       "44412  2.000000  \n",
       "31424  2.164689  \n",
       "27959  2.164689  \n",
       "4062   2.164689  \n",
       "41259  2.164689  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_train.info()\n",
    "subs_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
